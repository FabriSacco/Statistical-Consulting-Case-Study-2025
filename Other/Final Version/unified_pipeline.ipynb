{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **UnifiedDataPipeline** class handles all data processing tasks, integrating things from our individual work:\n",
    "\n",
    "- Load and preprocess startup data\n",
    "\n",
    "- Integrate external data sources (market data, economic data, industry risk, regulatory data, World Governance Indicators data)\n",
    "\n",
    "- Feature engineering:\n",
    "    - Time-based features: company age, funding duration, days since last funding\n",
    "    - Funding features: average round size, funding momentum\n",
    "    - Location features: country risk scores (based on regulatory framework), regional risk averages (for countries not in the framework)\n",
    "    - Industry features: category count, industry risk scores (based on predefined mappings), risk scores for different sectors\n",
    "    - Macro features: economic indicators (GDP, inflation, unemployment)\n",
    "    - URL features: domain length, HTTPS presence, URL complexity\n",
    "\n",
    "- Preprocess data for ML:\n",
    "    - Missing value imputation\n",
    "    - Feature scaling\n",
    "    - Outlier handling\n",
    "    - Data type conversion\n",
    "    - Special handling for different types of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabri\\Desktop\\uni\\MSDS\\2024-2025\\Semester 2\\Statistical Consulting\\Statistical-Consulting-Case-Study-2025\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from datetime import datetime, date\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import yaml\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import pycountry\n",
    "import zipfile\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedDataPipeline:\n",
    "    # Class constants\n",
    "    TODAY = pd.Timestamp(date.today())\n",
    "    EXTERNAL_DATA_DIR = Path(\"Fabri/External Data\")\n",
    "    ROMAIN_DATA_DIR = Path(\"Romain/ExternalData\")\n",
    "    DATA_DIR = Path(\"Data\")\n",
    "    CACHE_DIR = Path(\".cache\")\n",
    "    \n",
    "    def __init__(self, config_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the unified data pipeline\n",
    "        \n",
    "        Args:\n",
    "            config_path: Path to configuration file (optional)\n",
    "        \"\"\"\n",
    "        self.CACHE_DIR.mkdir(exist_ok=True)\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.external_data = {}\n",
    "        self.feature_columns = []\n",
    "        self.wgi_data = None  # Store WGI data\n",
    "        \n",
    "    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Load configuration from YAML file or use defaults\"\"\"\n",
    "        default_config = {\n",
    "            \"data_sources\": {\n",
    "                \"startup_data\": str(self.ROMAIN_DATA_DIR / \"startup_failures.csv\"),\n",
    "                \"market_data\": [\"SP500\", \"NASDAQ\", \"DowJones\", \"Volatility\"],\n",
    "                \"economic_data\": [\"gdp\", \"inflation\", \"unemployment\"],\n",
    "                \"industry_data\": str(self.EXTERNAL_DATA_DIR / \"industry_risk_profiles.json\"),\n",
    "                \"regulatory_data\": str(self.EXTERNAL_DATA_DIR / \"regulatory_framework.json\")\n",
    "            },\n",
    "            \"feature_engineering\": {\n",
    "                \"date_features\": True,\n",
    "                \"funding_features\": True,\n",
    "                \"location_features\": True,\n",
    "                \"industry_features\": True,\n",
    "                \"macro_features\": True,\n",
    "                \"url_features\": True\n",
    "            },\n",
    "            \"preprocessing\": {\n",
    "                \"imputation_strategy\": \"median\",\n",
    "                \"scaling\": True,\n",
    "                \"outlier_handling\": \"winsorize\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if config_path:\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "                return {**default_config, **config}\n",
    "        return default_config\n",
    "\n",
    "    def load_external_data(self) -> None:\n",
    "        \"\"\"Load all required external data sources\"\"\"\n",
    "        # Market data (from Fabri)\n",
    "        for index in self.config[\"data_sources\"][\"market_data\"]:\n",
    "            try:\n",
    "                df = pd.read_csv(self.EXTERNAL_DATA_DIR / f\"{index}_data.csv\")\n",
    "                self.external_data[f\"market_{index}\"] = df\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: {index} data not found\")\n",
    "\n",
    "        # Economic data (from Romain)\n",
    "        for indicator in self.config[\"data_sources\"][\"economic_data\"]:\n",
    "            try:\n",
    "                df = pd.read_csv(self.ROMAIN_DATA_DIR / f\"{indicator}.csv\")\n",
    "                self.external_data[f\"economic_{indicator}\"] = df\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: {indicator} data not found\")\n",
    "\n",
    "        # Industry and regulatory data (from Fabri)\n",
    "        try:\n",
    "            with open(self.EXTERNAL_DATA_DIR / \"industry_risk_profiles.json\", 'r') as f:\n",
    "                self.external_data[\"industry_risk\"] = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(\"Warning: Industry risk profiles not found\")\n",
    "\n",
    "        # Load regulatory framework\n",
    "        try:\n",
    "            with open(self.EXTERNAL_DATA_DIR / \"regulatory_framework.json\", 'r') as f:\n",
    "                regulatory_data = json.load(f)\n",
    "                # Ensure we have the expected structure\n",
    "                if 'country_risk' not in regulatory_data:\n",
    "                    regulatory_data['country_risk'] = {}\n",
    "                self.external_data[\"regulatory_framework\"] = regulatory_data\n",
    "        except FileNotFoundError:\n",
    "            print(\"Warning: Regulatory framework not found\")\n",
    "            self.external_data[\"regulatory_framework\"] = {\"country_risk\": {}}\n",
    "\n",
    "    def load_startup_data(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and clean startup data\"\"\"\n",
    "        # Define historical dates that should be treated as null values\n",
    "        historical_dates = [\n",
    "            '1899-12-31', '0000-00-00', '', '1871-01-01', '1869-01-01',\n",
    "            '1900-01-01', '1800-01-01', '1700-01-01'\n",
    "        ]\n",
    "        \n",
    "        # Load data using polars for better performance (from Romain)\n",
    "        pl_df = pl.read_csv(\n",
    "            file_path,\n",
    "            try_parse_dates=True,\n",
    "            null_values=historical_dates,\n",
    "            dtypes={\n",
    "                'founded_at': pl.Date,\n",
    "                'first_funding_at': pl.Date,\n",
    "                'last_funding_at': pl.Date,\n",
    "                'last_milestone_at': pl.Date,\n",
    "                'closed_at': pl.Date\n",
    "            },\n",
    "            infer_schema_length=10000,\n",
    "            ignore_errors=True\n",
    "        )\n",
    "        \n",
    "        # Convert to pandas for compatibility\n",
    "        df = pl_df.to_pandas()\n",
    "        \n",
    "        # Basic cleaning (from MayBrghl)\n",
    "        df = self._clean_funding_data(df)\n",
    "        df = self._process_dates(df)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _clean_funding_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean funding-related columns\"\"\"\n",
    "        funding_columns = [col for col in df.columns if 'funding' in col.lower()]\n",
    "        \n",
    "        for col in funding_columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                # Convert to numeric, replacing non-numeric values with NaN\n",
    "                df[col] = pd.to_numeric(\n",
    "                    df[col].str.replace('$', '').str.replace(',', ''), \n",
    "                    errors='coerce'\n",
    "                )\n",
    "            \n",
    "            # Handle funding_total_usd specifically\n",
    "            if col == 'funding_total_usd':\n",
    "                # Remove negative values\n",
    "                df[col] = df[col].clip(lower=0)\n",
    "                \n",
    "                # Calculate IQR for outlier detection\n",
    "                q1 = df[col].quantile(0.25)\n",
    "                q3 = df[col].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                upper_bound = q3 + 3 * iqr  # Using 3*IQR for more conservative outlier detection\n",
    "                \n",
    "                # Cap outliers at upper_bound\n",
    "                df[col] = df[col].clip(upper=upper_bound)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _process_dates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Process date columns\"\"\"\n",
    "        date_columns = [col for col in df.columns if 'date' in col.lower() or 'funding' in col.lower()]\n",
    "        \n",
    "        for col in date_columns:\n",
    "            if df[col].dtype == 'object':  # Only process if still string\n",
    "                # First try DD/MM/YYYY format\n",
    "                df[col] = pd.to_datetime(df[col], format='%d/%m/%Y', errors='coerce')\n",
    "                # If that fails, try other formats\n",
    "                if df[col].isna().any():\n",
    "                    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Engineer features from all three approaches\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Time-based features (from MayBrghl)\n",
    "        if self.config[\"feature_engineering\"][\"date_features\"]:\n",
    "            df = self._add_time_features(df)\n",
    "        \n",
    "        # Funding features (from Fabri)\n",
    "        if self.config[\"feature_engineering\"][\"funding_features\"]:\n",
    "            df = self._add_funding_features(df)\n",
    "        \n",
    "        # Location features (from Fabri)\n",
    "        if self.config[\"feature_engineering\"][\"location_features\"]:\n",
    "            df = self._add_location_features(df)\n",
    "        \n",
    "        # Industry features (from Fabri)\n",
    "        if self.config[\"feature_engineering\"][\"industry_features\"]:\n",
    "            df = self._add_industry_features(df)\n",
    "        \n",
    "        # Macro features (from Romain)\n",
    "        if self.config[\"feature_engineering\"][\"macro_features\"]:\n",
    "            df = self._add_macro_features(df)\n",
    "        \n",
    "        # URL features (from Fabri)\n",
    "        if self.config[\"feature_engineering\"][\"url_features\"]:\n",
    "            df = self._add_url_features(df)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _add_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add time-based features\"\"\"\n",
    "        # Company age - only calculate for rows with valid founded_at dates\n",
    "        valid_founded = df['founded_at'].notna()\n",
    "        df.loc[valid_founded, 'company_age'] = (\n",
    "            self.TODAY - df.loc[valid_founded, 'founded_at']\n",
    "        ).dt.days / 365.25\n",
    "        \n",
    "        # Ensure company age is non-negative\n",
    "        df['company_age'] = df['company_age'].clip(lower=0)\n",
    "        \n",
    "        # Funding duration - only calculate for rows with both first and last funding dates\n",
    "        valid_funding = df['first_funding_at'].notna() & df['last_funding_at'].notna()\n",
    "        df.loc[valid_funding, 'funding_duration_days'] = (\n",
    "            df.loc[valid_funding, 'last_funding_at'] - \n",
    "            df.loc[valid_funding, 'first_funding_at']\n",
    "        ).dt.days\n",
    "        \n",
    "        # Time since last funding - only calculate for rows with valid last_funding_at dates\n",
    "        valid_last_funding = df['last_funding_at'].notna()\n",
    "        df.loc[valid_last_funding, 'days_since_last_funding'] = (\n",
    "            self.TODAY - df.loc[valid_last_funding, 'last_funding_at']\n",
    "        ).dt.days\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _add_funding_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add funding-related features\"\"\"\n",
    "        # Average round size\n",
    "        df['average_round_size'] = df['funding_total_usd'] / df['funding_rounds']\n",
    "        \n",
    "        # Funding momentum\n",
    "        df['funding_momentum'] = df['funding_total_usd'] / df['company_age']\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _add_location_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add location-based features\"\"\"\n",
    "        # Combine location information\n",
    "        df['location_complete'] = df[['city', 'state_code', 'country_code']].fillna('').agg(', '.join, axis=1)\n",
    "        \n",
    "        # Load regulatory framework\n",
    "        try:\n",
    "            with open(self.EXTERNAL_DATA_DIR / \"regulatory_framework.json\", 'r') as f:\n",
    "                regulatory_data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(\"Warning: Regulatory framework not found, using default values\")\n",
    "            regulatory_data = {}\n",
    "        \n",
    "        def calculate_country_risk(country_code):\n",
    "            if not country_code:\n",
    "                return 0.5  # Default risk for missing country codes\n",
    "            \n",
    "            # Convert to uppercase for case-insensitive matching\n",
    "            country_code = str(country_code).upper()\n",
    "            \n",
    "            if country_code not in regulatory_data:\n",
    "                # For countries not in our framework, use regional averages\n",
    "                regional_averages = {\n",
    "                    'EUROPE': 0.25,  # Lower risk for European countries\n",
    "                    'ASIA': 0.35,    # Medium risk for Asian countries\n",
    "                    'AFRICA': 0.45,  # Higher risk for African countries\n",
    "                    'AMERICAS': 0.30, # Medium-low risk for American countries\n",
    "                    'OCEANIA': 0.25  # Lower risk for Oceanian countries\n",
    "                }\n",
    "                \n",
    "                # Try to get the region from pycountry\n",
    "                try:\n",
    "                    country = pycountry.countries.get(alpha_3=country_code)\n",
    "                    if country:\n",
    "                        # Map to our regions\n",
    "                        if country.alpha_2 in ['AT', 'BE', 'BG', 'HR', 'CY', 'CZ', 'DK', 'EE', 'FI', 'FR', 'DE', 'GR', 'HU', 'IE', 'IT', 'LV', 'LT', 'LU', 'MT', 'NL', 'PL', 'PT', 'RO', 'SK', 'SI', 'ES', 'SE']:\n",
    "                            return regional_averages['EUROPE']\n",
    "                        elif country.alpha_2 in ['JP', 'KR', 'CN', 'IN', 'ID', 'MY', 'PH', 'SG', 'TH', 'VN', 'TW', 'HK']:\n",
    "                            return regional_averages['ASIA']\n",
    "                        elif country.alpha_2 in ['NG', 'KE', 'GH', 'UG', 'TZ', 'ET', 'ZA']:\n",
    "                            return regional_averages['AFRICA']\n",
    "                        elif country.alpha_2 in ['US', 'CA', 'MX', 'BR', 'AR', 'CL', 'CO', 'PE']:\n",
    "                            return regional_averages['AMERICAS']\n",
    "                        elif country.alpha_2 in ['AU', 'NZ']:\n",
    "                            return regional_averages['OCEANIA']\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                return 0.5  # Default risk for unknown countries\n",
    "            \n",
    "            country_data = regulatory_data[country_code]\n",
    "            # Calculate risk score as weighted average of regulatory metrics\n",
    "            # Higher regulatory quality, rule of law, and control of corruption = lower risk\n",
    "            risk_score = (\n",
    "                country_data['regulatory_quality'] * 0.3 +\n",
    "                country_data['rule_of_law'] * 0.3 +\n",
    "                country_data['control_of_corruption'] * 0.4\n",
    "            )\n",
    "            return 1 - risk_score  # Convert to risk score (higher score = higher risk)\n",
    "        \n",
    "        df['country_risk_score'] = df['country_code'].apply(calculate_country_risk)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _load_wgi_data(self) -> None:\n",
    "        \"\"\"Download and process World Bank's Worldwide Governance Indicators\"\"\"\n",
    "        wgi_cache_file = self.CACHE_DIR / \"wgi_data.csv\"\n",
    "        \n",
    "        # Check if we have cached data\n",
    "        if wgi_cache_file.exists():\n",
    "            print(\"Loading cached WGI data...\")\n",
    "            try:\n",
    "                self.wgi_data = pd.read_csv(wgi_cache_file)\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading cached WGI data: {str(e)}\")\n",
    "                # If cache is corrupted, continue to download\n",
    "        \n",
    "        # World Bank API endpoints for WGI indicators\n",
    "        indicators = {\n",
    "            'Regulatory Quality': 'RQ.EST',\n",
    "            'Rule of Law': 'RL.EST',\n",
    "            'Control of Corruption': 'CC.EST'\n",
    "        }\n",
    "        \n",
    "        print(\"Downloading WGI data from World Bank API...\")\n",
    "        try:\n",
    "            all_data = []\n",
    "            \n",
    "            for indicator_name, indicator_code in indicators.items():\n",
    "                # World Bank API URL\n",
    "                api_url = f\"http://api.worldbank.org/v2/country/all/indicator/{indicator_code}?format=json&per_page=1000&date=2020:2023\"\n",
    "                \n",
    "                response = requests.get(api_url)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                if not data or len(data) < 2:\n",
    "                    print(f\"No data found for {indicator_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract the data\n",
    "                for entry in data[1]:\n",
    "                    all_data.append({\n",
    "                        'country_code': entry['countryiso3code'],\n",
    "                        'year': entry['date'],\n",
    "                        'indicator': indicator_name,\n",
    "                        'value': entry['value']\n",
    "                    })\n",
    "            \n",
    "            if not all_data:\n",
    "                print(\"No WGI data found\")\n",
    "                self.wgi_data = None\n",
    "                return\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(all_data)\n",
    "            \n",
    "            # Pivot the data to get indicators as columns\n",
    "            self.wgi_data = df.pivot_table(\n",
    "                index=['country_code', 'year'],\n",
    "                columns='indicator',\n",
    "                values='value'\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Normalize values from [-2.5, 2.5] to [0, 1]\n",
    "            for col in indicators.keys():\n",
    "                if col in self.wgi_data.columns:\n",
    "                    self.wgi_data[col] = (self.wgi_data[col] + 2.5) / 5.0\n",
    "            \n",
    "            # Save processed data to cache\n",
    "            self.wgi_data.to_csv(wgi_cache_file, index=False)\n",
    "            print(f\"Successfully downloaded and processed WGI data for {len(self.wgi_data)} countries\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading WGI data: {str(e)}\")\n",
    "            print(\"Using default regulatory framework instead\")\n",
    "            self.wgi_data = None\n",
    "\n",
    "    def _process_wgi_data(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Process WGI data into our regulatory framework format\"\"\"\n",
    "        if self.wgi_data is None:\n",
    "            return {}\n",
    "            \n",
    "        try:\n",
    "            # Try to find the year column (might be 'Year' or 'year')\n",
    "            year_col = None\n",
    "            for col in self.wgi_data.columns:\n",
    "                if col.lower() == 'year':\n",
    "                    year_col = col\n",
    "                    break\n",
    "            \n",
    "            if not year_col:\n",
    "                print(\"Warning: Year column not found in WGI data\")\n",
    "                return {}\n",
    "            \n",
    "            # Select relevant indicators\n",
    "            indicators = {\n",
    "                'Regulatory Quality': 'regulatory_quality',\n",
    "                'Rule of Law': 'rule_of_law',\n",
    "                'Control of Corruption': 'control_of_corruption'\n",
    "            }\n",
    "            \n",
    "            # Get the latest year's data\n",
    "            latest_year = self.wgi_data[year_col].max()\n",
    "            latest_data = self.wgi_data[self.wgi_data[year_col] == latest_year]\n",
    "            \n",
    "            # Try to find country code column\n",
    "            country_code_col = None\n",
    "            possible_cols = ['Country Code', 'country_code', 'ISO', 'iso']\n",
    "            for col in self.wgi_data.columns:\n",
    "                if col in possible_cols or col.lower() in [c.lower() for c in possible_cols]:\n",
    "                    country_code_col = col\n",
    "                    break\n",
    "            \n",
    "            if not country_code_col:\n",
    "                print(\"Warning: Country code column not found in WGI data\")\n",
    "                return {}\n",
    "            \n",
    "            regulatory_data = {}\n",
    "            \n",
    "            for _, row in latest_data.iterrows():\n",
    "                country_code = row[country_code_col]\n",
    "                if pd.isna(country_code):\n",
    "                    continue\n",
    "                    \n",
    "                country_data = {}\n",
    "                for wgi_indicator, our_indicator in indicators.items():\n",
    "                    # Try different possible column names\n",
    "                    value = None\n",
    "                    for col in self.wgi_data.columns:\n",
    "                        if wgi_indicator.lower() in col.lower():\n",
    "                            value = row[col]\n",
    "                            break\n",
    "                    \n",
    "                    if value is None or pd.isna(value):\n",
    "                        continue\n",
    "                        \n",
    "                    # Normalize from [-2.5, 2.5] to [0, 1]\n",
    "                    try:\n",
    "                        value = float(value)\n",
    "                        normalized_value = (value + 2.5) / 5.0\n",
    "                        normalized_value = max(0, min(1, normalized_value))  # Ensure value is between 0 and 1\n",
    "                        country_data[our_indicator] = normalized_value\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "                \n",
    "                if country_data:  # Only add if we have data\n",
    "                    regulatory_data[str(country_code).upper()] = country_data\n",
    "            \n",
    "            if not regulatory_data:\n",
    "                print(\"Warning: No valid regulatory data found in WGI dataset\")\n",
    "            else:\n",
    "                print(f\"Successfully processed WGI data for {len(regulatory_data)} countries\")\n",
    "                \n",
    "            return regulatory_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing WGI data: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _add_industry_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add industry-related features\"\"\"\n",
    "        # Split categories\n",
    "        df['category_list'] = df['category_list'].fillna('')\n",
    "        \n",
    "        # Count categories\n",
    "        df['category_count'] = df['category_list'].str.count('|') + 1\n",
    "        \n",
    "        # Create a mapping from categories to risk scores\n",
    "        industry_risk = self.external_data.get('industry_risk', {})\n",
    "        risk_mapping = {\n",
    "            # Technology sector\n",
    "            'software': ['software', 'apps', 'application', 'saas', 'enterprise', 'programming'],\n",
    "            'hardware': ['hardware', 'electronics', 'semiconductor', 'computer', 'mobile'],\n",
    "            \n",
    "            # Healthcare sector\n",
    "            'biotech': ['biotech', 'biotechnology', 'pharmaceutical', 'life science', 'medical device'],\n",
    "            'healthcare_services': ['health', 'medical', 'healthcare', 'hospital', 'wellness'],\n",
    "            \n",
    "            # Financial sector\n",
    "            'fintech': ['fintech', 'financial technology', 'payment', 'blockchain', 'cryptocurrency'],\n",
    "            'traditional_banking': ['banking', 'insurance', 'investment', 'finance'],\n",
    "            \n",
    "            # Retail sector\n",
    "            'e_commerce': ['e-commerce', 'ecommerce', 'marketplace', 'online shopping', 'online retail'],\n",
    "            'traditional_retail': ['retail', 'store', 'shopping', 'consumer goods']\n",
    "        }\n",
    "        \n",
    "        # Define risk scores for each category\n",
    "        base_risk_scores = {\n",
    "            'software': 0.05,\n",
    "            'hardware': 0.06,\n",
    "            'biotech': 0.07,\n",
    "            'healthcare_services': 0.03,\n",
    "            'fintech': 0.06,\n",
    "            'traditional_banking': 0.02,\n",
    "            'e_commerce': 0.08,\n",
    "            'traditional_retail': 0.04\n",
    "        }\n",
    "        \n",
    "        # Function to calculate risk score for a category string\n",
    "        def get_risk_score(category_string):\n",
    "            if not category_string:\n",
    "                return 0.05  # Default risk for empty categories\n",
    "            \n",
    "            categories = category_string.lower().split('|')\n",
    "            matched_risks = []\n",
    "            \n",
    "            for cat in categories:\n",
    "                cat = cat.strip()\n",
    "                matched = False\n",
    "                \n",
    "                # Try to match the category with our defined mappings\n",
    "                for industry_type, keywords in risk_mapping.items():\n",
    "                    if any(keyword in cat for keyword in keywords):\n",
    "                        matched_risks.append(base_risk_scores[industry_type])\n",
    "                        matched = True\n",
    "                        break\n",
    "                \n",
    "                # If no match found, assign risk based on some heuristics\n",
    "                if not matched:\n",
    "                    # High-risk categories\n",
    "                    if any(word in cat for word in ['game', 'social', 'media', 'entertainment']):\n",
    "                        matched_risks.append(0.07)  # Higher risk for consumer-facing entertainment\n",
    "                    # Medium-high risk categories\n",
    "                    elif any(word in cat for word in ['internet', 'digital', 'platform', 'technology']):\n",
    "                        matched_risks.append(0.06)  # Technology-related\n",
    "                    # Medium risk categories\n",
    "                    elif any(word in cat for word in ['service', 'consulting', 'marketing']):\n",
    "                        matched_risks.append(0.05)  # Service-based\n",
    "                    # Medium-low risk categories\n",
    "                    elif any(word in cat for word in ['education', 'training', 'research']):\n",
    "                        matched_risks.append(0.04)  # Education and research\n",
    "                    # Low risk categories\n",
    "                    elif any(word in cat for word in ['infrastructure', 'utility', 'government']):\n",
    "                        matched_risks.append(0.03)  # Infrastructure and utilities\n",
    "                    else:\n",
    "                        matched_risks.append(0.05)  # Default risk for unknown categories\n",
    "            \n",
    "            # Calculate weighted average risk score\n",
    "            # Give more weight to higher risks as they're more likely to impact the company\n",
    "            if matched_risks:\n",
    "                return np.average(matched_risks, weights=range(1, len(matched_risks) + 1))\n",
    "            return 0.05\n",
    "        \n",
    "        # Calculate risk scores\n",
    "        df['industry_risk_score'] = df['category_list'].apply(get_risk_score)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\nIndustry Risk Score Summary:\")\n",
    "        print(df['industry_risk_score'].describe())\n",
    "        \n",
    "        # Print some example mappings\n",
    "        print(\"\\nExample Category to Risk Score Mappings:\")\n",
    "        sample_categories = df['category_list'].sample(n=5, random_state=42)\n",
    "        for cat in sample_categories:\n",
    "            print(f\"\\nCategory: {cat}\")\n",
    "            print(f\"Risk Score: {get_risk_score(cat)}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _add_macro_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add macroeconomic features\"\"\"\n",
    "        # Check if country_code column exists, if not try alternative names\n",
    "        country_col = None\n",
    "        for col in ['country_code', 'country', 'countryCode', 'country_code']:\n",
    "            if col in df.columns:\n",
    "                country_col = col\n",
    "                break\n",
    "        \n",
    "        if country_col is None:\n",
    "            print(\"Warning: No country code column found in the dataset\")\n",
    "            return df\n",
    "            \n",
    "        # Extract year from founded_at date\n",
    "        df['year'] = pd.to_numeric(df['founded_at'].dt.year, errors='coerce')\n",
    "            \n",
    "        # Join with economic data\n",
    "        for indicator in self.config[\"data_sources\"][\"economic_data\"]:\n",
    "            eco_data = self.external_data.get(f\"economic_{indicator}\")\n",
    "            if eco_data is not None:\n",
    "                try:\n",
    "                    # Skip the first row which contains the indicator name\n",
    "                    eco_data = eco_data.iloc[1:]\n",
    "                    \n",
    "                    # Get the first column name (usually contains country names)\n",
    "                    country_column = eco_data.columns[0]\n",
    "                    \n",
    "                    # Convert wide format to long format\n",
    "                    eco_data_long = pd.melt(\n",
    "                        eco_data,\n",
    "                        id_vars=[country_column],\n",
    "                        var_name='year',\n",
    "                        value_name=indicator\n",
    "                    )\n",
    "                    \n",
    "                    # Clean up the data\n",
    "                    eco_data_long[indicator] = eco_data_long[indicator].replace('no data', np.nan)\n",
    "                    eco_data_long[indicator] = pd.to_numeric(eco_data_long[indicator], errors='coerce')\n",
    "                    eco_data_long['year'] = pd.to_numeric(eco_data_long['year'], errors='coerce')\n",
    "                    \n",
    "                    # Remove rows with invalid years\n",
    "                    eco_data_long = eco_data_long[eco_data_long['year'].notna()]\n",
    "                    \n",
    "                    # Create a mapping dictionary for country names to codes\n",
    "                    country_mapping = {}\n",
    "                    for idx, row in df.iterrows():\n",
    "                        if pd.notna(row[country_col]) and pd.notna(row.get('country_name')):\n",
    "                            country_mapping[row['country_name']] = row[country_col]\n",
    "                    \n",
    "                    # If no mapping was created, try to create one from the economic data\n",
    "                    if not country_mapping:\n",
    "                        # Get unique country names from both dataframes\n",
    "                        eco_countries = pd.Series(eco_data_long[country_column].unique())\n",
    "                        df_countries = pd.Series(df[country_col].unique())\n",
    "                        unique_countries = pd.concat([eco_countries, df_countries]).unique()\n",
    "                        \n",
    "                        # Try to map country names to codes\n",
    "                        for country in unique_countries:\n",
    "                            if pd.notna(country):\n",
    "                                try:\n",
    "                                    # Try to get the country code from pycountry\n",
    "                                    country_obj = pycountry.countries.get(name=country)\n",
    "                                    if country_obj is None:\n",
    "                                        country_obj = pycountry.countries.get(official_name=country)\n",
    "                                    if country_obj is not None:\n",
    "                                        country_mapping[country] = country_obj.alpha_3\n",
    "                                except:\n",
    "                                    continue\n",
    "                    \n",
    "                    # Map country names to codes and ensure consistent type\n",
    "                    eco_data_long['country_code'] = eco_data_long[country_column].map(country_mapping)\n",
    "                    eco_data_long['country_code'] = eco_data_long['country_code'].astype(str)\n",
    "                    \n",
    "                    # Convert the main dataframe's country_code to string type\n",
    "                    df[country_col] = df[country_col].astype(str)\n",
    "                    \n",
    "                    # Print debug information\n",
    "                    print(f\"\\nProcessing {indicator} data:\")\n",
    "                    print(f\"Unique countries in economic data: {eco_data_long[country_column].nunique()}\")\n",
    "                    print(f\"Unique country codes in economic data: {eco_data_long['country_code'].nunique()}\")\n",
    "                    print(f\"Unique country codes in main data: {df[country_col].nunique()}\")\n",
    "                    \n",
    "                    # Merge with the main dataframe\n",
    "                    df = df.merge(\n",
    "                        eco_data_long[['country_code', 'year', indicator]],\n",
    "                        left_on=[country_col, 'year'],\n",
    "                        right_on=['country_code', 'year'],\n",
    "                        how='left'\n",
    "                    )\n",
    "                    \n",
    "                    # Drop the redundant country_code column from the merge\n",
    "                    if 'country_code_y' in df.columns:\n",
    "                        df = df.drop('country_code_y', axis=1)\n",
    "                    \n",
    "                    # Print merge results\n",
    "                    print(f\"Successfully merged {indicator} data\")\n",
    "                    print(f\"Number of non-null values: {df[indicator].notna().sum()}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to merge {indicator} data: {str(e)}\")\n",
    "                    print(f\"Error details: {type(e).__name__}: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "        # Drop the temporary year column\n",
    "        df = df.drop('year', axis=1)\n",
    "                    \n",
    "        return df\n",
    "\n",
    "    def _add_url_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add URL-based features\"\"\"\n",
    "        url_cols = [col for col in df.columns if 'url' in col.lower() or 'homepage' in col.lower()]\n",
    "        \n",
    "        if url_cols:\n",
    "            url_col = url_cols[0]\n",
    "            # Handle None values and convert to empty string\n",
    "            df[url_col] = df[url_col].fillna('')\n",
    "            \n",
    "            # Add URL-based features\n",
    "            df['domain_length'] = df[url_col].str.len()\n",
    "            \n",
    "            # More robust HTTPS detection\n",
    "            df['has_https'] = df[url_col].str.lower().str.startswith('https://').fillna(False).astype(int)\n",
    "            \n",
    "            # More robust www detection\n",
    "            df['has_www'] = df[url_col].str.lower().str.contains(r'^https?://(?:www\\.)').fillna(False).astype(int)\n",
    "            \n",
    "            # More accurate URL complexity calculation\n",
    "            df['url_complexity'] = df[url_col].str.count(r'[^/]/').fillna(0)\n",
    "            # Cap complexity at a reasonable value\n",
    "            df['url_complexity'] = df['url_complexity'].clip(upper=10)\n",
    "            \n",
    "            # Extract domain with better pattern matching\n",
    "            df['domain'] = df[url_col].str.extract(r'https?://(?:www\\.)?([^/]+)')\n",
    "            # Clean up domain names\n",
    "            df['domain'] = df['domain'].str.lower().str.strip()\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess the data\"\"\"\n",
    "        # Convert category_count to integer\n",
    "        if 'category_count' in df.columns:\n",
    "            df['category_count'] = df['category_count'].fillna(0).astype(int)\n",
    "        \n",
    "        # Get all numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "        print(f\"Found {len(numeric_cols)} numeric columns: {numeric_cols}\")\n",
    "        \n",
    "        # Define columns to handle separately (not with standard imputation/scaling)\n",
    "        special_handling_cols = {\n",
    "            'zero_fill': [\n",
    "                'funding_total_usd', 'funding_rounds', 'average_round_size', \n",
    "                'funding_momentum', 'domain_length', 'has_https', 'has_www', \n",
    "                'url_complexity', 'category_count'\n",
    "            ],\n",
    "            'median_fill': [\n",
    "                'company_age', 'funding_duration_days', 'days_since_last_funding',\n",
    "                'gdp', 'inflation', 'unemployment', 'industry_risk_score', 'country_risk_score'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Handle zero-fill columns\n",
    "        for col in special_handling_cols['zero_fill']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(0)\n",
    "                if col in ['funding_rounds', 'category_count', 'has_https', 'has_www']:\n",
    "                    df[col] = df[col].astype(int)\n",
    "        \n",
    "        # Handle median-fill columns\n",
    "        for col in special_handling_cols['median_fill']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        # Get remaining numeric columns for standard processing\n",
    "        special_cols = [col for group in special_handling_cols.values() for col in group]\n",
    "        standard_numeric_cols = [col for col in numeric_cols if col not in special_cols and col != 'country_risk_score']\n",
    "        \n",
    "        if standard_numeric_cols:\n",
    "            print(f\"Processing standard numeric columns: {standard_numeric_cols}\")\n",
    "            numeric_df = df[standard_numeric_cols].copy()\n",
    "            \n",
    "            # Handle missing values\n",
    "            imputer = SimpleImputer(strategy=self.config[\"preprocessing\"][\"imputation_strategy\"])\n",
    "            imputed_values = imputer.fit_transform(numeric_df)\n",
    "            \n",
    "            # Scale numeric features\n",
    "            if self.config[\"preprocessing\"][\"scaling\"]:\n",
    "                scaler = StandardScaler()\n",
    "                scaled_values = scaler.fit_transform(imputed_values)\n",
    "                numeric_df = pd.DataFrame(\n",
    "                    scaled_values,\n",
    "                    columns=standard_numeric_cols,\n",
    "                    index=numeric_df.index\n",
    "                )\n",
    "                \n",
    "                # Handle outliers\n",
    "                if self.config[\"preprocessing\"][\"outlier_handling\"] == \"winsorize\":\n",
    "                    numeric_df = self._winsorize_outliers(numeric_df, standard_numeric_cols)\n",
    "                \n",
    "                # Update the original DataFrame\n",
    "                df[standard_numeric_cols] = numeric_df\n",
    "        \n",
    "        # Handle missing values in location data\n",
    "        location_cols = ['state_code', 'region', 'city']\n",
    "        for col in location_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna('Unknown')\n",
    "        \n",
    "        # Handle missing values in domain\n",
    "        if 'domain' in df.columns:\n",
    "            df['domain'] = df['domain'].fillna('')\n",
    "        \n",
    "        # Handle missing values in dates\n",
    "        date_cols = ['founded_at', 'first_funding_at', 'last_funding_at']\n",
    "        for col in date_cols:\n",
    "            if col in df.columns:\n",
    "                if col == 'founded_at':\n",
    "                    # For founded_at, use a default date if missing\n",
    "                    df[col] = df[col].fillna(pd.Timestamp('2000-01-01'))\n",
    "                else:\n",
    "                    # For funding dates, use the last available date\n",
    "                    df[col] = df[col].fillna(df[col].max())\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _winsorize_outliers(self, df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Winsorize outliers in specified columns\"\"\"\n",
    "        for col in columns:\n",
    "            q1 = df[col].quantile(0.25)\n",
    "            q3 = df[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            \n",
    "            df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def process(self, startup_data_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Complete data processing pipeline\n",
    "        \n",
    "        Args:\n",
    "            startup_data_path: Path to startup data file\n",
    "            \n",
    "        Returns:\n",
    "            Processed DataFrame ready for modeling\n",
    "        \"\"\"\n",
    "        # Load external data\n",
    "        self.load_external_data()\n",
    "        \n",
    "        # Load and clean startup data\n",
    "        df = self.load_startup_data(startup_data_path)\n",
    "        \n",
    "        # Engineer features\n",
    "        df = self.engineer_features(df)\n",
    "        \n",
    "        # Preprocess data\n",
    "        df = self.preprocess_data(df)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Industry Risk Score Summary:\n",
      "count    66368.000000\n",
      "mean         0.052947\n",
      "std          0.010010\n",
      "min          0.020000\n",
      "25%          0.050000\n",
      "50%          0.050000\n",
      "75%          0.058000\n",
      "max          0.080000\n",
      "Name: industry_risk_score, dtype: float64\n",
      "\n",
      "Example Category to Risk Score Mappings:\n",
      "\n",
      "Category: Agriculture|Apps|Information Technology|Internet|Mobile|Social Entrepreneurship|Telecommunications\n",
      "Risk Score: 0.05857142857142857\n",
      "\n",
      "Category: Software\n",
      "Risk Score: 0.05\n",
      "\n",
      "Category: Finance\n",
      "Risk Score: 0.02\n",
      "\n",
      "Category: Health and Wellness\n",
      "Risk Score: 0.03\n",
      "\n",
      "Category: Electronics\n",
      "Risk Score: 0.06\n",
      "\n",
      "Processing gdp data:\n",
      "Unique countries in economic data: 227\n",
      "Unique country codes in economic data: 174\n",
      "Unique country codes in main data: 138\n",
      "Successfully merged gdp data\n",
      "Number of non-null values: 45543\n",
      "\n",
      "Processing inflation data:\n",
      "Unique countries in economic data: 228\n",
      "Unique country codes in economic data: 174\n",
      "Unique country codes in main data: 138\n",
      "Successfully merged inflation data\n",
      "Number of non-null values: 45515\n",
      "\n",
      "Processing unemployment data:\n",
      "Unique countries in economic data: 119\n",
      "Unique country codes in economic data: 99\n",
      "Unique country codes in main data: 138\n",
      "Successfully merged unemployment data\n",
      "Number of non-null values: 44040\n",
      "Found 17 numeric columns: ['funding_total_usd', 'funding_rounds', 'company_age', 'funding_duration_days', 'days_since_last_funding', 'average_round_size', 'funding_momentum', 'country_risk_score', 'category_count', 'industry_risk_score', 'gdp', 'inflation', 'unemployment', 'domain_length', 'has_https', 'has_www', 'url_complexity']\n",
      "\n",
      "Processed data saved to processed_data.parquet (parquet format)\n",
      "Processed data saved to processed_data.csv (CSV format)\n",
      "\n",
      "Summary of processed data:\n",
      "Total rows: 66,368\n",
      "Total columns: 31\n",
      "\n",
      "Numeric columns summary:\n",
      "       funding_total_usd  funding_rounds   company_age  funding_duration_days  \\\n",
      "count       6.636800e+04    66368.000000  66368.000000           66368.000000   \n",
      "mean        6.681044e+06        1.732522     16.498563             342.081621   \n",
      "std         1.155480e+07        1.360251      7.023939             710.242708   \n",
      "min         0.000000e+00        1.000000      0.000000               0.000000   \n",
      "25%         4.000000e+04        1.000000     13.325120               0.000000   \n",
      "50%         1.000000e+06        1.000000     14.658453               0.000000   \n",
      "75%         6.800473e+06        2.000000     17.325120             415.000000   \n",
      "max         3.899313e+07       19.000000    124.323066           36994.000000   \n",
      "\n",
      "       days_since_last_funding  average_round_size  funding_momentum  \\\n",
      "count             66368.000000        6.636800e+04      6.636800e+04   \n",
      "mean               4689.094338        3.629937e+06               inf   \n",
      "std                1248.641549        6.992001e+06               NaN   \n",
      "min              -33087.000000        0.000000e+00      0.000000e+00   \n",
      "25%                3816.000000        3.765400e+04      0.000000e+00   \n",
      "50%                4258.000000        6.500000e+05      1.531575e+04   \n",
      "75%                5201.000000        4.000000e+06      2.193347e+05   \n",
      "max               19842.000000        3.899313e+07               inf   \n",
      "\n",
      "       country_risk_score  category_count  industry_risk_score            gdp  \\\n",
      "count        66368.000000    66368.000000         66368.000000   66368.000000   \n",
      "mean             0.218871       29.508634             0.052947   43820.339053   \n",
      "std              0.152282       25.335456             0.010010   10831.058311   \n",
      "min              0.085008        2.000000             0.020000     525.894000   \n",
      "25%              0.129000       12.000000             0.050000   41236.687000   \n",
      "50%              0.129000       21.000000             0.050000   47102.428000   \n",
      "75%              0.265170       41.000000             0.058000   48586.288000   \n",
      "max              0.692751      676.000000             0.080000  111437.514000   \n",
      "\n",
      "          inflation  unemployment  domain_length     has_https       has_www  \\\n",
      "count  66368.000000  66368.000000   66368.000000  66368.000000  66368.000000   \n",
      "mean       2.387134      7.288910      21.904728      0.038106      0.616336   \n",
      "std       13.954203      2.272938       8.150649      0.191453      0.486281   \n",
      "min       -3.400000      0.700000       0.000000      0.000000      0.000000   \n",
      "25%        1.700000      6.200000      20.000000      0.000000      0.000000   \n",
      "50%        1.900000      7.400000      23.000000      0.000000      1.000000   \n",
      "75%        2.600000      8.000000      26.000000      0.000000      1.000000   \n",
      "max     2477.100000     31.600000     263.000000      1.000000      1.000000   \n",
      "\n",
      "       url_complexity  \n",
      "count    66368.000000  \n",
      "mean         1.134734  \n",
      "std          0.563146  \n",
      "min          0.000000  \n",
      "25%          1.000000  \n",
      "50%          1.000000  \n",
      "75%          1.000000  \n",
      "max          9.000000  \n",
      "\n",
      "Available features:\n",
      "- permalink\n",
      "- name\n",
      "- homepage_url\n",
      "- category_list\n",
      "- funding_total_usd\n",
      "- status\n",
      "- country_code\n",
      "- state_code\n",
      "- region\n",
      "- city\n",
      "- funding_rounds\n",
      "- founded_at\n",
      "- first_funding_at\n",
      "- last_funding_at\n",
      "- company_age\n",
      "- funding_duration_days\n",
      "- days_since_last_funding\n",
      "- average_round_size\n",
      "- funding_momentum\n",
      "- location_complete\n",
      "- country_risk_score\n",
      "- category_count\n",
      "- industry_risk_score\n",
      "- gdp\n",
      "- inflation\n",
      "- unemployment\n",
      "- domain_length\n",
      "- has_https\n",
      "- has_www\n",
      "- url_complexity\n",
      "- domain\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    pipeline = UnifiedDataPipeline()\n",
    "    processed_data = pipeline.process(str(UnifiedDataPipeline.ROMAIN_DATA_DIR / \"startup_failures.csv\"))\n",
    "    \n",
    "    # Save processed data in both formats\n",
    "    parquet_path = \"processed_data.parquet\"\n",
    "    csv_path = \"processed_data.csv\"\n",
    "    \n",
    "    # Save as parquet\n",
    "    processed_data.to_parquet(parquet_path, index=False)\n",
    "    print(f\"\\nProcessed data saved to {parquet_path} (parquet format)\")\n",
    "    \n",
    "    # Save as CSV\n",
    "    processed_data.to_csv(csv_path, index=False)\n",
    "    print(f\"Processed data saved to {csv_path} (CSV format)\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary of processed data:\")\n",
    "    print(f\"Total rows: {len(processed_data):,}\")\n",
    "    print(f\"Total columns: {len(processed_data.columns):,}\")\n",
    "    print(\"\\nNumeric columns summary:\")\n",
    "    numeric_cols = processed_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    print(processed_data[numeric_cols].describe())\n",
    "    \n",
    "    print(\"\\nAvailable features:\")\n",
    "    for col in processed_data.columns:\n",
    "        print(f\"- {col}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Credit Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CreditRiskML** class handles all ML tasks for credit risk assessment:\n",
    "\n",
    "- Feature preparation:\n",
    "    - Financial: total funding, funding rounds, average funding per round, frequency of funding rounds, funding growth rate, funding efficiency relative to company age, funding stability\n",
    "    - Temporal: company age, duration of funding period, time since last funding round, ratio of time since last funding to company age, recency of funding events, rate of funding rounds\n",
    "    - Geographic: country-specific risk score, GDP, inflation rate, unemployment rate, combined economic stability\n",
    "    - Industry: industry-specific risk score, number of business categories, industry concentration\n",
    "    - Operational: URL complexity, HTTPS presence, WWW presence, domain name length, combined digital presence score\n",
    "\n",
    "- Model training and evaluation:\n",
    "    - Random Forest with class weights to handle class imbalance (weights are inversely proportional to their frequencies in the input data, giving more importance to the minority class, namely the \"default\" class)\n",
    "    - Stratified 5-fold cross-validation (model was trained on 4/5 of the data and evaluated on 1/5 of the data, with the final model trained on the entire dataset)\n",
    "    - Performance metrics: precision, recall, F1 score, ROC AUC, confusion matrix\n",
    "\n",
    "- Risk and credit decisions:\n",
    "    - Risk category: low, medium, high, very high\n",
    "    - Credit decision: approve, approve with conditions, review, decline\n",
    "    - Credit limits: none, low, medium, high\n",
    "    - Monitoring frequency: none, weekly, monthly, quarterly\n",
    "\n",
    "- Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, classification_report,\n",
    "    roc_curve, precision_score, recall_score, f1_score,\n",
    "    average_precision_score, accuracy_score, confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import shap\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CreditRiskML:\n",
    "    def __init__(self, model_dir: str = \"models\"):\n",
    "        \"\"\"\n",
    "        Initialize the ML credit risk assessment system\n",
    "        \n",
    "        Args:\n",
    "            model_dir: Directory to save trained models\n",
    "        \"\"\"\n",
    "        self.model_dir = Path(model_dir)\n",
    "        self.model_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Define feature groups\n",
    "        self.feature_groups = {\n",
    "            'financial': [\n",
    "                'funding_total_usd',\n",
    "                'funding_rounds',\n",
    "                'average_round_size',\n",
    "                'funding_momentum'\n",
    "            ],\n",
    "            'temporal': [\n",
    "                'company_age',\n",
    "                'funding_duration_days',\n",
    "                'days_since_last_funding'\n",
    "            ],\n",
    "            'geographic': [\n",
    "                'country_risk_score',\n",
    "                'gdp',\n",
    "                'inflation',\n",
    "                'unemployment'\n",
    "            ],\n",
    "            'industry': [\n",
    "                'industry_risk_score',\n",
    "                'category_count'\n",
    "            ],\n",
    "            'operational': [\n",
    "                'url_complexity',\n",
    "                'has_https',\n",
    "                'has_www',\n",
    "                'domain_length'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Initialize models\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        \n",
    "    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare features for model training\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame with raw features\n",
    "            \n",
    "        Returns:\n",
    "            Processed DataFrame with prepared features\n",
    "        \"\"\"\n",
    "        # Create feature groups\n",
    "        for group, features in self.feature_groups.items():\n",
    "            # Check if all features exist\n",
    "            missing_features = [f for f in features if f not in df.columns]\n",
    "            if missing_features:\n",
    "                logger.warning(f\"Missing features for {group}: {missing_features}\")\n",
    "                continue\n",
    "            \n",
    "            # Create group-specific features\n",
    "            if group == 'financial':\n",
    "                df['funding_efficiency'] = df['funding_total_usd'] / (df['company_age'] + 1)\n",
    "                df['funding_stability'] = df['funding_rounds'] / (df['company_age'] + 1)\n",
    "            \n",
    "            elif group == 'temporal':\n",
    "                df['funding_recency'] = 1 / (df['days_since_last_funding'] + 1)\n",
    "                df['funding_velocity'] = df['funding_rounds'] / df['funding_duration_days']\n",
    "            \n",
    "            elif group == 'geographic':\n",
    "                df['economic_stability'] = (df['gdp'] - df['inflation']) / (df['unemployment'] + 1)\n",
    "            \n",
    "            elif group == 'industry':\n",
    "                df['industry_concentration'] = 1 / (df['category_count'] + 1)\n",
    "            \n",
    "            elif group == 'operational':\n",
    "                df['digital_presence'] = (\n",
    "                    df['has_https'].astype(int) + \n",
    "                    df['has_www'].astype(int) + \n",
    "                    (df['url_complexity'] > 0).astype(int)\n",
    "                ) / 3\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def train_models(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"Train models with memory optimization\"\"\"\n",
    "        try:\n",
    "            # Clear memory\n",
    "            gc.collect()\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Train model\n",
    "            logger.info(\"Training model...\")\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=10,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Store models and scalers\n",
    "            self.model = model\n",
    "            self.scaler = scaler\n",
    "            \n",
    "            # Calculate feature importance\n",
    "            self.feature_importance = dict(zip(\n",
    "                X.columns,\n",
    "                model.feature_importances_\n",
    "            ))\n",
    "            \n",
    "            # Save models\n",
    "            self._save_models()\n",
    "            \n",
    "            # Evaluate model\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            logger.info(\"\\nClassification Report:\")\n",
    "            logger.info(classification_report(y_test, y_pred))\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during model training: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def predict_risk(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Predict risk scores for new data\"\"\"\n",
    "        try:\n",
    "            if self.model is None or self.scaler is None:\n",
    "                raise ValueError(\"Model or scaler not initialized. Please train the model first.\")\n",
    "                \n",
    "            # Scale features\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            \n",
    "            # Get predictions\n",
    "            risk_scores = self.model.predict_proba(X_scaled)[:, 1]\n",
    "            \n",
    "            # Create risk categories\n",
    "            risk_categories = pd.cut(\n",
    "                risk_scores,\n",
    "                bins=[0, 0.2, 0.5, 0.8, 1],\n",
    "                labels=['Low', 'Medium', 'High', 'Very High']\n",
    "            )\n",
    "            \n",
    "            # Create output DataFrame\n",
    "            results = pd.DataFrame({\n",
    "                'risk_score': risk_scores,\n",
    "                'risk_category': risk_categories\n",
    "            })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during prediction: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def make_credit_decisions(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Make credit decisions based on risk predictions\n",
    "        \n",
    "        Args:\n",
    "            X: Input DataFrame with preprocessed numeric features\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with credit decisions\n",
    "        \"\"\"\n",
    "        # Get risk predictions\n",
    "        risk_predictions = self.predict_risk(X)\n",
    "        if risk_predictions is None:\n",
    "            return None\n",
    "            \n",
    "        # Define decision rules\n",
    "        def get_credit_decision(row):\n",
    "            if row['risk_category'] == 'Low':\n",
    "                return {\n",
    "                    'decision': 'APPROVE',\n",
    "                    'credit_limit': 'HIGH',\n",
    "                    'monitoring_frequency': 'QUARTERLY'\n",
    "                }\n",
    "            elif row['risk_category'] == 'Medium':\n",
    "                return {\n",
    "                    'decision': 'APPROVE_WITH_CONDITIONS',\n",
    "                    'credit_limit': 'MEDIUM',\n",
    "                    'monitoring_frequency': 'MONTHLY'\n",
    "                }\n",
    "            elif row['risk_category'] == 'High':\n",
    "                return {\n",
    "                    'decision': 'REVIEW',\n",
    "                    'credit_limit': 'LOW',\n",
    "                    'monitoring_frequency': 'WEEKLY'\n",
    "                }\n",
    "            else:  # Very High\n",
    "                return {\n",
    "                    'decision': 'DECLINE',\n",
    "                    'credit_limit': 'NONE',\n",
    "                    'monitoring_frequency': 'NONE'\n",
    "                }\n",
    "        \n",
    "        # Apply decision rules\n",
    "        decisions = risk_predictions.apply(get_credit_decision, axis=1)\n",
    "        decisions_df = pd.DataFrame(decisions.tolist())\n",
    "        \n",
    "        # Combine with risk predictions\n",
    "        results = pd.concat([risk_predictions, decisions_df], axis=1)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _save_models(self):\n",
    "        \"\"\"Save trained models to disk\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        model_path = self.model_dir / f\"rf_{timestamp}.joblib\"\n",
    "        joblib.dump(self.model, model_path)\n",
    "        logger.info(f\"Saved model to {model_path}\")\n",
    "        \n",
    "        # Convert feature importance values to float64 for JSON serialization\n",
    "        importance_dict = {\n",
    "            'feature_importance': {\n",
    "                str(k): float(v) for k, v in self.feature_importance.items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save feature importance\n",
    "        importance_path = self.model_dir / f\"feature_importance_{timestamp}.json\"\n",
    "        with open(importance_path, 'w') as f:\n",
    "            json.dump(importance_dict, f)\n",
    "        logger.info(f\"Saved feature importance to {importance_path}\")\n",
    "    \n",
    "    def load_models(self, timestamp: str):\n",
    "        \"\"\"\n",
    "        Load trained models from disk\n",
    "        \n",
    "        Args:\n",
    "            timestamp: Timestamp of the models to load\n",
    "        \"\"\"\n",
    "        for name in ['xgb', 'cox', 'anomaly']:\n",
    "            model_path = self.model_dir / f\"{name}_{timestamp}.joblib\"\n",
    "            if model_path.exists():\n",
    "                self.model = joblib.load(model_path)\n",
    "                logger.info(f\"Loaded {name} model from {model_path}\")\n",
    "        \n",
    "        # Load feature importance\n",
    "        importance_path = self.model_dir / f\"feature_importance_{timestamp}.json\"\n",
    "        if importance_path.exists():\n",
    "            with open(importance_path, 'r') as f:\n",
    "                self.feature_importance = json.load(f)\n",
    "            logger.info(f\"Loaded feature importance from {importance_path}\")\n",
    "    \n",
    "    def evaluate_models(self, X: pd.DataFrame, y: pd.Series) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate model performance\n",
    "        \n",
    "        Args:\n",
    "            X: Test data features\n",
    "            y: True labels\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get predictions\n",
    "            predictions = self.predict_risk(X)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                'auc_roc': roc_auc_score(y, predictions['risk_score']),\n",
    "                'risk_distribution': predictions['risk_category'].value_counts(normalize=True).to_dict()\n",
    "            }\n",
    "            \n",
    "            # Calculate confusion matrix\n",
    "            y_pred = (predictions['risk_score'] > 0.5).astype(int)\n",
    "            cm = confusion_matrix(y, y_pred)\n",
    "            \n",
    "            # Create confusion matrix visualization\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                        xticklabels=['Non-Default', 'Default'],\n",
    "                        yticklabels=['Non-Default', 'Default'])\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            \n",
    "            # Ensure metrics directory exists\n",
    "            metrics_dir = Path(\"output/metrics\")\n",
    "            metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Save confusion matrix plot\n",
    "            cm_path = metrics_dir / \"confusion_matrix.png\"\n",
    "            plt.savefig(cm_path, bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "            logger.info(f\"Saved confusion matrix to {cm_path}\")\n",
    "            \n",
    "            # Print classification report\n",
    "            logger.info(\"\\nClassification Report:\")\n",
    "            logger.info(classification_report(\n",
    "                y,\n",
    "                predictions['risk_score'] > 0.5\n",
    "            ))\n",
    "            \n",
    "            # Print confusion matrix\n",
    "            logger.info(\"\\nConfusion Matrix:\")\n",
    "            logger.info(cm)\n",
    "            \n",
    "            # Print risk distribution\n",
    "            logger.info(\"\\nRisk Distribution:\")\n",
    "            for category, proportion in metrics['risk_distribution'].items():\n",
    "                logger.info(f\"{category}: {proportion:.2%}\")\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in model evaluation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def plot_risk_distribution(self, predictions: pd.DataFrame, save_path: str = None):\n",
    "        \"\"\"\n",
    "        Plot the distribution of risk scores and categories\n",
    "        \n",
    "        Args:\n",
    "            predictions: DataFrame with risk predictions\n",
    "            save_path: Optional path to save the plot\n",
    "        \"\"\"\n",
    "        fig = make_subplots(rows=2, cols=1, subplot_titles=('Risk Score Distribution', 'Risk Category Distribution'))\n",
    "        \n",
    "        # Risk score distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=predictions['risk_score'], name='Risk Score'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Risk category distribution\n",
    "        category_counts = predictions['risk_category'].value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=category_counts.index, y=category_counts.values, name='Risk Categories'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=800, title_text=\"Risk Distribution Analysis\")\n",
    "        if save_path:\n",
    "            fig.write_html(save_path)\n",
    "        return fig\n",
    "    \n",
    "    def plot_feature_importance(self, save_path: str = None):\n",
    "        \"\"\"\n",
    "        Plot feature importance from the XGBoost model\n",
    "        \n",
    "        Args:\n",
    "            save_path: Optional path to save the plot\n",
    "        \"\"\"\n",
    "        if 'xgb' not in self.feature_importance:\n",
    "            logger.warning(\"No feature importance data available\")\n",
    "            return None\n",
    "        \n",
    "        importance_data = pd.DataFrame({\n",
    "            'feature': list(self.feature_importance['xgb'].keys()),\n",
    "            'importance': list(self.feature_importance['xgb'].values())\n",
    "        }).sort_values('importance', ascending=True)\n",
    "        \n",
    "        fig = px.bar(\n",
    "            importance_data,\n",
    "            x='importance',\n",
    "            y='feature',\n",
    "            orientation='h',\n",
    "            title='Feature Importance'\n",
    "        )\n",
    "        \n",
    "        if save_path:\n",
    "            fig.write_html(save_path)\n",
    "        return fig\n",
    "    \n",
    "    def plot_shap_summary(self, X: pd.DataFrame, save_path: str = None):\n",
    "        \"\"\"\n",
    "        Create SHAP summary plot to explain model predictions\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix\n",
    "            save_path: Optional path to save the plot\n",
    "        \"\"\"\n",
    "        if 'xgb' not in self.model:\n",
    "            logger.warning(\"No trained XGBoost model available\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        explainer = shap.TreeExplainer(self.model)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        \n",
    "        # Create summary plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
    "        plt.title(\"SHAP Feature Importance\")\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    def plot_risk_by_feature(self, df: pd.DataFrame, feature: str, save_path: str = None):\n",
    "        \"\"\"\n",
    "        Plot risk scores against a specific feature\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            feature: Feature to plot against\n",
    "            save_path: Optional path to save the plot\n",
    "        \"\"\"\n",
    "        predictions = self.predict_risk(df)\n",
    "        combined = pd.concat([df[[feature]], predictions], axis=1)\n",
    "        \n",
    "        fig = px.scatter(\n",
    "            combined,\n",
    "            x=feature,\n",
    "            y='risk_score',\n",
    "            color='risk_category',\n",
    "            title=f'Risk Score by {feature}',\n",
    "            labels={'risk_score': 'Risk Score', feature: feature}\n",
    "        )\n",
    "        \n",
    "        if save_path:\n",
    "            fig.write_html(save_path)\n",
    "        return fig\n",
    "    \n",
    "    def create_risk_dashboard(self, df: pd.DataFrame, save_dir: str = \"risk_dashboard\"):\n",
    "        \"\"\"\n",
    "        Create a comprehensive risk dashboard\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            save_dir: Directory to save dashboard components\n",
    "        \"\"\"\n",
    "        save_dir = Path(save_dir)\n",
    "        save_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.predict_risk(df)\n",
    "        \n",
    "        # Create various plots\n",
    "        self.plot_risk_distribution(\n",
    "            predictions,\n",
    "            save_path=str(save_dir / \"risk_distribution.html\")\n",
    "        )\n",
    "        \n",
    "        self.plot_feature_importance(\n",
    "            save_path=str(save_dir / \"feature_importance.html\")\n",
    "        )\n",
    "        \n",
    "        # Plot risk by key features\n",
    "        for feature in ['funding_total_usd', 'company_age', 'country_risk_score']:\n",
    "            if feature in df.columns:\n",
    "                self.plot_risk_by_feature(\n",
    "                    df,\n",
    "                    feature,\n",
    "                    save_path=str(save_dir / f\"risk_by_{feature}.html\")\n",
    "                )\n",
    "        \n",
    "        # Create SHAP summary plot\n",
    "        X = self.prepare_features(df)\n",
    "        self.plot_shap_summary(\n",
    "            X,\n",
    "            save_path=str(save_dir / \"shap_summary.png\")\n",
    "        )\n",
    "        \n",
    "        # Save predictions\n",
    "        predictions.to_csv(save_dir / \"risk_predictions.csv\", index=False)\n",
    "        \n",
    "        logger.info(f\"Risk dashboard created in {save_dir}\")\n",
    "\n",
    "    def explain_prediction(self, df: pd.DataFrame, index: int) -> dict:\n",
    "        \"\"\"\n",
    "        Explain the prediction for a specific instance\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            index: Index of the instance to explain\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with explanation details\n",
    "        \"\"\"\n",
    "        if 'xgb' not in self.model:\n",
    "            logger.warning(\"No trained XGBoost model available\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare features\n",
    "        X = self.prepare_features(df)\n",
    "        instance = X.iloc[[index]]\n",
    "        \n",
    "        # Get SHAP values\n",
    "        explainer = shap.TreeExplainer(self.model)\n",
    "        shap_values = explainer.shap_values(instance)\n",
    "        \n",
    "        # Get prediction details\n",
    "        prediction = self.predict_risk(df.iloc[[index]])\n",
    "        \n",
    "        # Create explanation\n",
    "        explanation = {\n",
    "            'risk_score': float(prediction['risk_score'].iloc[0]),\n",
    "            'risk_category': prediction['risk_category'].iloc[0],\n",
    "            'feature_contributions': dict(zip(\n",
    "                X.columns,\n",
    "                shap_values[0]  # For binary classification, we use the first class\n",
    "            )),\n",
    "            'base_value': float(explainer.expected_value[0]),\n",
    "            'instance_features': instance.iloc[0].to_dict()\n",
    "        }\n",
    "        \n",
    "        return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 12:25:24,825 - __main__ - INFO - Loading data...\n",
      "2025-04-29 12:25:24,922 - __main__ - INFO - Class weights: {np.int64(0): np.float64(0.5518709462830533), np.int64(1): np.float64(5.31965373517153)}\n",
      "2025-04-29 12:25:24,926 - __main__ - INFO - Performing cross-validation...\n",
      "2025-04-29 12:25:30,663 - __main__ - INFO - \n",
      "Fold 1 Results:\n",
      "2025-04-29 12:25:30,679 - __main__ - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.86      0.90     12026\n",
      "           1       0.28      0.50      0.36      1248\n",
      "\n",
      "    accuracy                           0.83     13274\n",
      "   macro avg       0.61      0.68      0.63     13274\n",
      "weighted avg       0.88      0.83      0.85     13274\n",
      "\n",
      "2025-04-29 12:25:30,679 - __main__ - INFO - \n",
      "Confusion Matrix:\n",
      "2025-04-29 12:25:30,679 - __main__ - INFO - [[10376  1650]\n",
      " [  621   627]]\n",
      "2025-04-29 12:25:36,486 - __main__ - INFO - \n",
      "Fold 2 Results:\n",
      "2025-04-29 12:25:36,501 - __main__ - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90     12026\n",
      "           1       0.28      0.55      0.37      1248\n",
      "\n",
      "    accuracy                           0.83     13274\n",
      "   macro avg       0.61      0.70      0.64     13274\n",
      "weighted avg       0.89      0.83      0.85     13274\n",
      "\n",
      "2025-04-29 12:25:36,501 - __main__ - INFO - \n",
      "Confusion Matrix:\n",
      "2025-04-29 12:25:36,501 - __main__ - INFO - [[10283  1743]\n",
      " [  566   682]]\n",
      "2025-04-29 12:25:42,446 - __main__ - INFO - \n",
      "Fold 3 Results:\n",
      "2025-04-29 12:25:42,463 - __main__ - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90     12026\n",
      "           1       0.28      0.52      0.37      1248\n",
      "\n",
      "    accuracy                           0.83     13274\n",
      "   macro avg       0.61      0.69      0.63     13274\n",
      "weighted avg       0.88      0.83      0.85     13274\n",
      "\n",
      "2025-04-29 12:25:42,463 - __main__ - INFO - \n",
      "Confusion Matrix:\n",
      "2025-04-29 12:25:42,463 - __main__ - INFO - [[10370  1656]\n",
      " [  594   654]]\n",
      "2025-04-29 12:25:47,703 - __main__ - INFO - \n",
      "Fold 4 Results:\n",
      "2025-04-29 12:25:47,714 - __main__ - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.90     12026\n",
      "           1       0.29      0.52      0.37      1247\n",
      "\n",
      "    accuracy                           0.83     13273\n",
      "   macro avg       0.62      0.69      0.64     13273\n",
      "weighted avg       0.88      0.83      0.85     13273\n",
      "\n",
      "2025-04-29 12:25:47,714 - __main__ - INFO - \n",
      "Confusion Matrix:\n",
      "2025-04-29 12:25:47,715 - __main__ - INFO - [[10427  1599]\n",
      " [  604   643]]\n",
      "2025-04-29 12:25:52,546 - __main__ - INFO - \n",
      "Fold 5 Results:\n",
      "2025-04-29 12:25:52,562 - __main__ - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90     12026\n",
      "           1       0.27      0.53      0.36      1247\n",
      "\n",
      "    accuracy                           0.82     13273\n",
      "   macro avg       0.61      0.69      0.63     13273\n",
      "weighted avg       0.88      0.82      0.85     13273\n",
      "\n",
      "2025-04-29 12:25:52,562 - __main__ - INFO - \n",
      "Confusion Matrix:\n",
      "2025-04-29 12:25:52,566 - __main__ - INFO - [[10273  1753]\n",
      " [  584   663]]\n",
      "2025-04-29 12:25:52,905 - __main__ - INFO - \n",
      "Saved aggregated confusion matrix to output\\metrics\\confusion_matrix.png\n",
      "2025-04-29 12:25:52,906 - __main__ - INFO - \n",
      "Cross-validation Results:\n",
      "2025-04-29 12:25:52,907 - __main__ - INFO - accuracy: 0.829 (+/- 0.004)\n",
      "2025-04-29 12:25:52,908 - __main__ - INFO - precision: 0.280 (+/- 0.005)\n",
      "2025-04-29 12:25:52,909 - __main__ - INFO - recall: 0.524 (+/- 0.015)\n",
      "2025-04-29 12:25:52,910 - __main__ - INFO - f1: 0.365 (+/- 0.006)\n",
      "2025-04-29 12:25:52,912 - __main__ - INFO - roc_auc: 0.800 (+/- 0.004)\n",
      "2025-04-29 12:25:52,913 - __main__ - INFO - avg_precision: 0.314 (+/- 0.005)\n",
      "2025-04-29 12:25:59,980 - __main__ - INFO - Saved predictions to output\\ml_credit_risk_predictions.csv\n",
      "2025-04-29 12:26:06,369 - __main__ - INFO - ML processing completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions Summary:\n",
      "================================================================================\n",
      "Total companies analyzed: 66368\n",
      "\n",
      "Risk Distribution:\n",
      "risk_category\n",
      "Low          49.38\n",
      "Medium       30.94\n",
      "High         17.53\n",
      "Very High     2.16\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Credit Decision Distribution:\n",
      "decision\n",
      "APPROVE                    49.38\n",
      "APPROVE_WITH_CONDITIONS    30.94\n",
      "REVIEW                     17.53\n",
      "DECLINE                     2.16\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Sample of predictions (first 5 companies):\n",
      "             company_name country     status  risk_score risk_category  \\\n",
      "0                   #fame     IND  operating    0.106189           Low   \n",
      "1                :Qounter     USA  operating    0.129574           Low   \n",
      "2  (THE) ONE of THEM,Inc.    None  operating    0.316706        Medium   \n",
      "3                 0-6.com     CHN  operating    0.422246        Medium   \n",
      "4        004 Technologies     USA  operating    0.051348           Low   \n",
      "\n",
      "                  decision credit_limit monitoring_frequency  \n",
      "0                  APPROVE         HIGH            QUARTERLY  \n",
      "1                  APPROVE         HIGH            QUARTERLY  \n",
      "2  APPROVE_WITH_CONDITIONS       MEDIUM              MONTHLY  \n",
      "3  APPROVE_WITH_CONDITIONS       MEDIUM              MONTHLY  \n",
      "4                  APPROVE         HIGH            QUARTERLY  \n",
      "\n",
      "Performance visualizations have been saved to: output\\metrics\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load data with memory optimization\n",
    "        logger.info(\"Loading data...\")\n",
    "        df = pd.read_parquet(\n",
    "            \"output/processed_data.parquet\",\n",
    "            columns=[\n",
    "                'country_risk_score', 'industry_risk_score', 'funding_total_usd', \n",
    "                'status', 'name', 'country_code', 'company_age', 'funding_rounds',\n",
    "                'days_since_last_funding', 'category_count', 'funding_duration_days'\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Calculate default status from status column\n",
    "        df['default_status'] = (df['status'] == 'closed').astype(int)\n",
    "        \n",
    "        # Create more informative features\n",
    "        df['funding_per_round'] = df['funding_total_usd'] / (df['funding_rounds'] + 1)\n",
    "        df['funding_frequency'] = df['funding_rounds'] / (df['funding_duration_days'] + 1)\n",
    "        df['time_since_funding_ratio'] = df['days_since_last_funding'] / (df['company_age'] + 1)\n",
    "        \n",
    "        # Select features for modeling\n",
    "        feature_cols = [\n",
    "            'country_risk_score', 'industry_risk_score', 'funding_total_usd',\n",
    "            'company_age', 'funding_rounds', 'days_since_last_funding',\n",
    "            'category_count', 'funding_duration_days', 'funding_per_round',\n",
    "            'funding_frequency', 'time_since_funding_ratio'\n",
    "        ]\n",
    "        \n",
    "        # Prepare feature matrix\n",
    "        X = df[feature_cols].copy()\n",
    "        y = df['default_status']\n",
    "        \n",
    "        # Handle missing values with median imputation\n",
    "        for col in X.columns:\n",
    "            X[col] = X[col].fillna(X[col].median())\n",
    "        \n",
    "        # Calculate class weights\n",
    "        n_samples = len(y)\n",
    "        n_classes = len(np.unique(y))\n",
    "        class_weights = dict(zip(\n",
    "            np.unique(y),\n",
    "            n_samples / (n_classes * np.bincount(y))\n",
    "        ))\n",
    "        \n",
    "        logger.info(\"Class weights: %s\", class_weights)\n",
    "        \n",
    "        # Initialize ML system\n",
    "        ml_system = CreditRiskML()\n",
    "        \n",
    "        # Initialize model with balanced class weights\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            min_samples_leaf=5,\n",
    "            class_weight=class_weights,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = {\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': [],\n",
    "            'roc_auc': [],\n",
    "            'avg_precision': []\n",
    "        }\n",
    "        \n",
    "        # Initialize confusion matrix for all folds\n",
    "        total_cm = np.zeros((2, 2))\n",
    "        \n",
    "        logger.info(\"Performing cross-validation...\")\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_val_scaled = scaler.transform(X_val)\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Get predictions\n",
    "            y_pred = model.predict(X_val_scaled)\n",
    "            y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "            \n",
    "            # Calculate confusion matrix for this fold\n",
    "            cm = confusion_matrix(y_val, y_pred)\n",
    "            total_cm += cm\n",
    "            \n",
    "            # Calculate metrics\n",
    "            cv_scores['accuracy'].append(accuracy_score(y_val, y_pred))\n",
    "            cv_scores['precision'].append(precision_score(y_val, y_pred, zero_division=0))\n",
    "            cv_scores['recall'].append(recall_score(y_val, y_pred))\n",
    "            cv_scores['f1'].append(f1_score(y_val, y_pred))\n",
    "            cv_scores['roc_auc'].append(roc_auc_score(y_val, y_pred_proba))\n",
    "            cv_scores['avg_precision'].append(average_precision_score(y_val, y_pred_proba))\n",
    "            \n",
    "            logger.info(f\"\\nFold {fold} Results:\")\n",
    "            logger.info(classification_report(y_val, y_pred))\n",
    "            logger.info(\"\\nConfusion Matrix:\")\n",
    "            logger.info(cm)\n",
    "        \n",
    "        # Create and save confusion matrix visualization for all folds\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(total_cm.astype(int), annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['Non-Default', 'Default'],\n",
    "                    yticklabels=['Non-Default', 'Default'])\n",
    "        plt.title('Aggregated Confusion Matrix (All Folds)')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        \n",
    "        # Save confusion matrix plot\n",
    "        metrics_dir = Path(\"output/metrics\")\n",
    "        metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "        cm_path = metrics_dir / \"confusion_matrix.png\"\n",
    "        plt.savefig(cm_path, bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "        logger.info(f\"\\nSaved aggregated confusion matrix to {cm_path}\")\n",
    "        \n",
    "        # Print cross-validation results\n",
    "        logger.info(\"\\nCross-validation Results:\")\n",
    "        for metric, scores in cv_scores.items():\n",
    "            logger.info(f\"{metric}: {np.mean(scores):.3f} (+/- {np.std(scores):.3f})\")\n",
    "        \n",
    "        # Train final model on full dataset\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        model.fit(X_scaled, y)\n",
    "        \n",
    "        # Store model and scaler in ml_system\n",
    "        ml_system.model = model\n",
    "        ml_system.scaler = scaler\n",
    "        \n",
    "        # Make predictions and credit decisions\n",
    "        credit_decisions = ml_system.make_credit_decisions(X)\n",
    "        \n",
    "        # Add company identifiers back to predictions\n",
    "        credit_decisions['company_name'] = df['name']\n",
    "        credit_decisions['country'] = df['country_code']\n",
    "        credit_decisions['status'] = df['status']\n",
    "        \n",
    "        # Save predictions\n",
    "        output_dir = Path(\"output\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        predictions_path = output_dir / \"ml_credit_risk_predictions.csv\"\n",
    "        credit_decisions.to_csv(predictions_path, index=False)\n",
    "        logger.info(f\"Saved predictions to {predictions_path}\")\n",
    "        \n",
    "        # Create performance metrics visualizations\n",
    "        metrics_dir = output_dir / \"metrics\"\n",
    "        metrics_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # ROC Curve\n",
    "        fig_roc = go.Figure()\n",
    "        for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "            X_val_scaled = scaler.transform(X.iloc[val_idx])\n",
    "            y_val = y.iloc[val_idx]\n",
    "            y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "            auc_score = roc_auc_score(y_val, y_pred_proba)\n",
    "            \n",
    "            fig_roc.add_trace(\n",
    "                go.Scatter(x=fpr, y=tpr, name=f'Fold {fold} (AUC = {auc_score:.3f})')\n",
    "            )\n",
    "        \n",
    "        fig_roc.add_trace(\n",
    "            go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Random', line=dict(dash='dash'))\n",
    "        )\n",
    "        \n",
    "        fig_roc.update_layout(\n",
    "            title='ROC Curves across CV Folds',\n",
    "            xaxis_title='False Positive Rate',\n",
    "            yaxis_title='True Positive Rate',\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig_roc.write_html(str(metrics_dir / \"roc_curves.html\"))\n",
    "        \n",
    "        # Precision-Recall Curve\n",
    "        fig_pr = go.Figure()\n",
    "        for fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n",
    "            X_val_scaled = scaler.transform(X.iloc[val_idx])\n",
    "            y_val = y.iloc[val_idx]\n",
    "            y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "            \n",
    "            precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n",
    "            avg_precision = average_precision_score(y_val, y_pred_proba)\n",
    "            \n",
    "            fig_pr.add_trace(\n",
    "                go.Scatter(x=recall, y=precision, name=f'Fold {fold} (AP = {avg_precision:.3f})')\n",
    "            )\n",
    "        \n",
    "        fig_pr.update_layout(\n",
    "            title='Precision-Recall Curves across CV Folds',\n",
    "            xaxis_title='Recall',\n",
    "            yaxis_title='Precision',\n",
    "            showlegend=True\n",
    "        )\n",
    "        fig_pr.write_html(str(metrics_dir / \"precision_recall_curves.html\"))\n",
    "        \n",
    "        # Feature Importance Plot\n",
    "        importance_data = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=True)\n",
    "        \n",
    "        fig_imp = px.bar(\n",
    "            importance_data,\n",
    "            x='importance',\n",
    "            y='feature',\n",
    "            orientation='h',\n",
    "            title='Feature Importance'\n",
    "        )\n",
    "        fig_imp.write_html(str(metrics_dir / \"feature_importance.html\"))\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nPredictions Summary:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total companies analyzed: {len(credit_decisions)}\")\n",
    "        print(\"\\nRisk Distribution:\")\n",
    "        print(credit_decisions['risk_category'].value_counts(normalize=True).mul(100).round(2))\n",
    "        print(\"\\nCredit Decision Distribution:\")\n",
    "        print(credit_decisions['decision'].value_counts(normalize=True).mul(100).round(2))\n",
    "        print(\"\\nSample of predictions (first 5 companies):\")\n",
    "        print(credit_decisions[['company_name', 'country', 'status', 'risk_score', 'risk_category', 'decision', 'credit_limit', 'monitoring_frequency']].head())\n",
    "        \n",
    "        print(\"\\nPerformance visualizations have been saved to:\", str(metrics_dir))\n",
    "        logger.info(\"ML processing completed!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "- Accuracy: The model correctly classifies about 83% of all cases\n",
    "\n",
    "- Precision: When the model predicts a company will default, it's correct about 28% of the time\n",
    "\n",
    "- Recall: The model correctly classifies about 52% of actual default cases\n",
    "\n",
    "- F1: The model achieves an F1 score of 36%, meaning it sacrifices precision to achieve better recall\n",
    "\n",
    "- ROC AUC: The model achieves a ROC AUC of 80%, indicating good discrimination ability\n",
    "\n",
    "- Confusion matrix:\n",
    "    - For non-defaulting companies, the model achieves high precision (94%), meaning it's usually correct when it predicts a company won't default, and good recall (86%), meaning it correctly identifies most safe companies\n",
    "    - For defaulting companies, the model achieves low precision (28%), meaning many companies predicted to default won't actually default, and moderate recall (50%), meaning it catches about half of the actual default cases\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "- The model is better at risk detection than precision\n",
    "\n",
    "- The model is better at identifying potentially risky companies (high recall) but makes more false positive predictions (low precision), hence might be appropriate for initial risk screening, where it's more important to catch potential risks than to be precise in every prediction\n",
    "\n",
    "- The model is stable and not overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
